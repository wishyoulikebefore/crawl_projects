import requests
from bs4 import BeautifulSoup
import time
import random
import re
import argparse
import io
import sys

"""
设定关键词：cancer sequencing mutation
url1 = "https://scholar.google.com.hk/scholar?hl=zh-CN&as_sdt=0%2C5&q=cancer+sequencing+mutation"
设定2014年起
url2 = "https://scholar.google.com.hk/scholar?as_ylo=2014&q=cancer+sequencing+mutation&hl=zh-CN&as_sdt=0,5"
按日期排序
url3 = "https://scholar.google.com.hk/scholar?hl=zh-CN&as_sdt=0,5&q=cancer+sequencing+mutation&scisbd=1"
修改页码
url4 = "https://scholar.google.com.hk/scholar?start=10&q=cancer+sequencing+mutation&hl=zh-CN&as_sdt=0,5"
#每页呈现10个结果
"""
"""
需要改进：
1）根据link信息补充文献来源，添加影响因子
2）统计作者信息，新增基于作者查询文献功能
3）切换到按日期排序，跟踪最新文章，根据影响因子筛选
"""
#cmd不能很好地兼容utf8，而IDLE就可以
sys.stdout = io.TextIOWrapper(sys.stdout.buffer,encoding='gb18030')

parser = argparse.ArgumentParser(description='Crawl google scholar')
parser.add_argument("-k",action='store',nargs="*")
parser.add_argument("-lim",type=int)
args = parser.parse_args()

keywords = "+".join(args.k)
pageLimit = args.lim

def getHtmlPage(page,cookies=None):
    url = "https://scholar.google.com.hk/scholar?start=%d&q=%s&hl=zh-CN&as_sdt=0,5" %(10*page,keywords)
    user_agents = [
        'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11',
        'Opera/9.25 (Windows NT 5.1; U; en)',
        'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)',
        'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)',
        'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12',
        'Lynx/2.8.5rel.1 libwww-FM/2.14 SSL-MM/1.4.1 GNUTLS/1.2.9',
        "Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Ubuntu/11.04 Chromium/16.0.912.77 Chrome/16.0.912.77 Safari/535.7",
        "Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0 "
    ]
    random_user_agent = random.choice(user_agents)
    headers = {
        'User-Agent': random_user_agent,
        "Connection": "keep - alive",
        "Accept":"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        }
    print("爬虫将要访问的网址为：%s" %(url))
    if cookies == None:
        html = requests.get(url,headers=headers)
    else:
        html = requests.get(url,headers=headers,cookies=cookies)
    soup = BeautifulSoup(html.text, 'lxml')
    return soup

for nu in range(pageLimit):
    soup = getHtmlPage(nu)
    targetItems = soup.find("div",id="gs_res_ccl").find_all("div",class_="gs_r gs_or gs_scl")
    for item in targetItems:
        try:
            paper = item.find("h3",class_="gs_rt").find("a").get_text()
            link = item.find("h3",class_="gs_rt").find("a")["href"]
            info = item.find("div",class_="gs_a").get_text()
            published_time = re.search("(\d+)",info)
            citeNum = re.search("被引用次数：\d+",str(item)).group(0).replace("被引用次数：","")
            print(paper)
            print(link)
            print(published_time.group(1))
            print(citeNum)
            print("------")
        except Exception as e:
            pass
    time.sleep(3+random.random())
